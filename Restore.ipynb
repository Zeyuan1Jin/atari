{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os.chdir(\"atari\")\n",
    "from run_dqn_atari import *\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "from atari_wrappers import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  ['device: 0, name: Tesla K80, pci bus id: 0000:00:04.0']\n"
     ]
    }
   ],
   "source": [
    "# Get Atari games.\n",
    "benchmark = gym.benchmark_spec('Atari40M')\n",
    "# Change the index to select a different game.\n",
    "task = benchmark.tasks[3]#Pong\n",
    "# task = benchmark.tasks[1]#breakout\n",
    "# task = benchmark.tasks[6]#spaceinvader\n",
    "# task = benchmark.tasks[4]#Qbert\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_env(task, seed)\n",
    "sess = get_session()\n",
    "num_timesteps=task.max_timesteps\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopping_criterion(env, t):\n",
    "    # notice that here t is the number of steps of the wrapped env,\n",
    "    # which is different from the number of steps in the underlying env\n",
    "    return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('pong.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "graph = tf.get_default_graph()\n",
    "#q_func=atari_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "# target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_q_func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.graph.get_operations()\n",
    "q_func_val= graph.get_tensor_by_name('target_q_func/action_value/fully_connected_1/BiasAdd:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_Input=graph.get_tensor_by_name(\"Placeholder:0\")\n",
    "img_Input1=graph.get_tensor_by_name(\"Placeholder_1:0\")\n",
    "img_Input2=graph.get_tensor_by_name(\"Placeholder_2:0\")\n",
    "img_Input3=graph.get_tensor_by_name(\"Placeholder_3:0\")\n",
    "img_Input4=graph.get_tensor_by_name(\"Placeholder_4:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_initialized = False\n",
    "num_param_updates = 0\n",
    "mean_episode_reward      = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "last_obs = env.reset()\n",
    "LOG_EVERY_N_STEPS = 1000\n",
    "\n",
    "rewards_record = []\n",
    "mean_episode_reward_record = []\n",
    "best_mean_episode_reward_record = []\n",
    "\n",
    "replay_buffer_size=2\n",
    "frame_history_len=4\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in itertools.count():\n",
    "    if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "        break \n",
    "         \n",
    "    if t == 0:\n",
    "        # initializing the buffer\n",
    "        ##step 1\n",
    "        idx = replay_buffer.store_frame(last_obs)  \n",
    "        act, reward, done = env.action_space.sample(), 0, False\n",
    "        last_obs, reward, done, info = env.step(act)\n",
    "        replay_buffer.store_effect(idx, act, reward, done)\n",
    "\n",
    "      \n",
    "    idx = replay_buffer.store_frame(last_obs) \n",
    "    \n",
    "    if idx==0:\n",
    "        id_current,id_next=1,0\n",
    "    else:\n",
    "        id_current,id_next=0,1\n",
    "    \n",
    "    obs_batch      = replay_buffer._encode_observation(id_current)[None]\n",
    "    act_batch      = replay_buffer.action[[id_current]]\n",
    "    rew_batch      = replay_buffer.reward[[id_current]]\n",
    "    next_obs_batch = replay_buffer._encode_observation(id_next)[None]\n",
    "    done_mask      = np.array([1.0 if replay_buffer.done[idx] else 0.0], dtype=np.float32)\n",
    "    \n",
    "    q_vals=sess.run(q_func_val,{img_Input:obs_batch,img_Input1:act_batch,img_Input2:rew_batch,img_Input3:next_obs_batch,img_Input4:done_mask})\n",
    "    act = np.argmax(q_vals)\n",
    "    last_obs, reward, done, info = env.step(act)\n",
    "    replay_buffer.store_effect(idx, act, reward, done) \n",
    "    \n",
    "    if done == True:\n",
    "        last_obs = env.reset()\n",
    "        done = False\n",
    "    \n",
    "    \n",
    "    episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "    if len(episode_rewards) > 0:\n",
    "        rewards_record.append(episode_rewards[-1])\n",
    "\n",
    "    if len(episode_rewards) > 0:\n",
    "        mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "        mean_episode_reward_record.append(mean_episode_reward)\n",
    "\n",
    "    if len(episode_rewards) > 100:\n",
    "        best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "        best_mean_episode_reward_record.append(best_mean_episode_reward)\n",
    "    \n",
    "    if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "        print(\"Timestep %d\" % (t,))\n",
    "        print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "        print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "        print(\"episodes %d\" % len(episode_rewards))\n",
    "        print(\"exploration %f\" % exploration.value(t))\n",
    "        print(\"learning_rate %f\" % optimizer_spec.lr_schedule.value(t))\n",
    "        sys.stdout.flush()\n",
    "        record = ({\"rewards\":rewards_record, \"mean_rewards\": mean_episode_reward_record, \"best_mean\": best_mean_episode_reward_record})\n",
    "        with open(\"./restore_rewards.pickle\", \"wb\") as f:\n",
    "            pickle.dump(record, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
