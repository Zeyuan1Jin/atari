{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import os\n",
    "os.chdir(\"atari\")\n",
    "from run_dqn_atari import *\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "from atari_wrappers import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./pong\n"
     ]
    }
   ],
   "source": [
    "# Get Atari games.\n",
    "benchmark = gym.benchmark_spec('Atari40M')\n",
    "# Change the index to select a different game.\n",
    "task = benchmark.tasks[3]\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_env(task, seed)\n",
    "sess = get_session()\n",
    "num_timesteps=task.max_timesteps\n",
    "   \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('pong.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "#q_func=atari_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_q_func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer_size=2\n",
    "frame_history_len=4\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "Argmax= graph.get_tensor_by_name(\"ArgMax:0\")\n",
    "img_Input=graph.get_tensor_by_name(\"Placeholder:0\")\n",
    "img_Input1=graph.get_tensor_by_name(\"Placeholder_1:0\")\n",
    "img_Input2=graph.get_tensor_by_name(\"Placeholder_2:0\")\n",
    "img_Input3=graph.get_tensor_by_name(\"Placeholder_3:0\")\n",
    "img_Input4=graph.get_tensor_by_name(\"Placeholder_4:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_initialized = False\n",
    "num_param_updates = 0\n",
    "mean_episode_reward      = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "last_obs = env.reset()\n",
    "LOG_EVERY_N_STEPS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in itertools.count():\n",
    "    if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "        break\n",
    "    idx = replay_buffer.store_frame(last_obs)   \n",
    "    if t == 0:\n",
    "        act, reward, done = env.action_space.sample(), 0, False\n",
    "        replay_buffer.store_effect(idx, act, reward, done)\n",
    "    last_obs, reward, done, info = env.step(act)\n",
    "    idx = replay_buffer.store_frame(last_obs) \n",
    "    replay_buffer.store_effect(idx, act, reward, done)\n",
    "    \n",
    "    obs_batch      = replay_buffer._encode_observation(0)[None]\n",
    "    act_batch      = replay_buffer.action[[0]]\n",
    "    rew_batch      = replay_buffer.reward[[0]]\n",
    "    next_obs_batch = replay_buffer._encode_observation(1)[None]\n",
    "    done_mask      = np.array([0.0])\n",
    "    \n",
    "    input_batch = replay_buffer._encode_sample(0)\n",
    "#     q_vals = session.run(q_func_1, {obs_t_ph: input_batch[None, :]})\n",
    "#     act = np.argmax(q_vals)\n",
    "#     obs_t_float   = tf.cast(input_batch[None, :],   tf.float32) / 255.0\n",
    "#     q_vals=q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "    \n",
    "    act=sess.run(Argmax,{img_Input:obs_batch,img_Input1:act_batch,img_Input2:rew_batch,img_Input3:next_obs_batch,img_Input4:done_mask})\n",
    "    \n",
    "    \n",
    "    \n",
    "    episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "    if len(episode_rewards) > 0:\n",
    "            rewards_record.append(episode_rewards[-1])\n",
    "\n",
    "    if len(episode_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "            mean_episode_reward_record.append(mean_episode_reward)\n",
    "\n",
    "    if len(episode_rewards) > 100:\n",
    "        best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "        best_mean_episode_reward_record.append(best_mean_episode_reward)\n",
    "    \n",
    "    if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "        print(\"Timestep %d\" % (t,))\n",
    "        print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "        print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "        print(\"episodes %d\" % len(episode_rewards))\n",
    "        print(\"exploration %f\" % exploration.value(t))\n",
    "        print(\"learning_rate %f\" % optimizer_spec.lr_schedule.value(t))\n",
    "        sys.stdout.flush()\n",
    "        record = ({\"rewards\":rewards_record, \"mean_rewards\": mean_episode_reward_record, \"best_mean\": best_mean_episode_reward_record})\n",
    "        with open(\"./rewards.pickle\", \"wb\") as f:\n",
    "            pickle.dump(record, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
